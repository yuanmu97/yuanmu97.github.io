<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Blog: InFi: End-to-End Learnable Input Filter for Resource-Efficient Mobile-Centric Inference</title>
    <link rel="stylesheet" href="../stylesheets/styles.css">
    <link rel="stylesheet" href="../stylesheets/pygment_trac.css">
    <link rel="icon" href="../imgs/favicon.png">
    <meta name="viewport" content="width=device-width">
  </head>
  
  <body>
    <div class="wrapper">
      <header>
        <a href="index.html"> <img src="blog-logo.png" alt="Logo" width="60%"/> </a>
        <p>Mu Yuan's Latest Research</p>
      </header>
      <section>
        <h1>InFi: End-to-End Learnable Input Filter for Resource-Efficient Mobile-Centric Inference</h1>
        <h2>April 17, 2022</h2>
        
        
        <p>
            The increased computing power of mobile devices and the growing demand for real-time sensor data analytics have created a trend of mobile-centric artificial intelligence.
            <a href="https://www.visualcapitalist.com/aiot-when-ai-meets-iot-technology/" target="_blank" rel="noopener noreferrer">Recent repots</a> estimat that over 80% of  enterprise IoT projects will incorporate AI by 2022.
            However, most AI models that achieve <a href="https://paperswithcode.com/sota" target="_blank" rel="noopener noreferrer">SOTA accuracy</a> are too computationally intensive to do inference with low latency on mobile devices. 
            The inference efficiency is still unsatisfying even when the workloads are offloaded to edge or cloud servers.
        </p>

        <p>
            Fortunately, redundant inputs widely exist in mobile-centric AI applications.
            Exitsing work explored two types of mechanism for input filtering: inference <a href="https://arxiv.org/abs/1905.13536" target="_blank" rel="noopener noreferrer">SKIP</a> and inference <a href="https://dl.acm.org/doi/10.1145/3241539.3241557" target="_blank" rel="noopener noreferrer">REUSE</a>.
            Inference SKIP aims to skip the inference computations that will not generate meaningful outputs, e.g., running face detector on photos without any face:
        </p>
        <figure>
            <img src="imgs/infi/redundancy_face.gif" width="80%"/>
        </figure>
        <p>
            Smart speakers may upload audios without any valid command to the cloud for speech recognition:
        </p>
        <figure>
            <img src="imgs/infi/redundancy_speech.gif" width="80%"/>
        </figure>
        <p>
            Inference REUSE tries to reuse obatined inference outputs, so as to reponse faster via a cache when new queries arrive.
            For example, the action classifier on smart bands may generate the same action label on similar motion signals:
        </p>
        <figure>
            <img src="imgs/infi/redundancy_motion.gif" width="80%"/>
        </figure>
        <p>
            Drone-based traffic monitoring applications may have the unchanged counting results on successive video frames:
        </p>
        <figure>
            <img src="imgs/infi/redundancy_traffic.gif" width="80%"/>
        </figure>
        <p>
            In this work, we study the input filtering problem with theoretical analysis for the first time and propose a uniform end-to-end learnable framework, InFi.
            This work has been accepted by ACM MobiCom 2022.
        </p>

        <b>Filterability Analysis</b>
        <p>
            Intuitively speaking, the filterability of an inference task refers to whether we can obtain an accurate predictor for the redundancy label using lower cost, compared with the original inference workload.
            The original inference model maps an input to its inference results, e.g., a face detection model takes an image as the input and outputs the bounding box of human faces.
            According to the outputs of inference model, we define a redundancy measurement function that outputs the redundancy label.
            For example, when there is no bounding box returned, the inference computation is seen as redundant.
            The difference between training the original model and training its filter is the supervision field: the original model is supervised by the groundtruth label (e.g., face bounding box) while the filter model is supervised by the redundancy label. 
            So one intuitive idea is, if it is easier to learn the filter than the original model, there is a potential to obtain an effective input filter.
        </p>
        <p>
            Based on this idea, we analyze three common inference tasks: classification with confidence-based redundancy measurement, classification with class-based redundancy measurement, and regression with threshold-based redundancy measurement.
            And we prove that the first task is not filterable, while the other two tasks are filterable, based on <a href="https://en.wikipedia.org/wiki/Rademacher_complexity" target="_blank" rel="noopener noreferrer">Rademarcher complexity</a> comparisons.
        </p>

        <b>Framework Design and Implementation</b>
        <p>
            The above filterability analysis is based on modeling the input filter as an end-to-end learning task.
            So our design needs to be end-to-end learnable, rather than being dependent on handcraft features or pre-trained deep features.
            Meanwhile, our design should supports both SKIP and REUSE mechanisms uniformly.
            Based on one neat insight, i.e., SKIP equals to REUSE inference results of an all-zero input, our design, InFi, unifies these two mechanisms.
        </p>
        <p>
            In the training phase, InFi uses a Siamese neural network to extract features for a pair of inputs, then computes the distance.
            A classifier neural network is used to predict the redundancy label.
        </p>
        <figure>
            <img src="imgs/infi/train.gif" width="100%"/>
        </figure>
        <p>
            In the inference phase, SKIP fixes the other input feature as zero, which degenerates the workflow into a base classifier.
            Based on the predicted redundancy label, InFi-Skip decides whether to skip the current input. 
            REUSE mechanism maintains a feature-result table as the cache.
            Through computing the distance between the current input's feature and cached ones, InFi-Reuse devices whether to reuse cached results via K-NN algorithm.
        </p>
        <figure>
            <img src="imgs/infi/inference.gif" width="100%"/>
        </figure>
        <p>
            We design modality-specific feature networks for text, image, video, audio, motion signal and feature map, and task-agnostic classifier.
            It is also easy to extand InFi to more data modalities.
            InFi also supports different deployment ways, including on-device, offloading, and model partitioning.
            InFi is open sourced in GitHub <a href="https://github.com/yuanmu97/infi" target="_blank" rel="noopener noreferrer">(link)</a>.
        </p>

        <b>Evaluation</b>
        <p>
            We evaluated InFi on 5 datasets, covering 12 inference tasks and 6 modalities (image, video, text. audio, motion signal and feature map).
            Take the vehicle counting task on videos as an example, compared with original workload, InFi-Skip and InFi-Reuse boost the throughput by 1.9 and 7.5 times, respectively.
            And both of them keep over 90% inference accuracy.
            Under the end-edge model partitioning inference, InFi-Skip and InFi-Reuse can save 70.7% and 95.0% communication bandwidth.
        </p>
        <p>
            InFi's training costs are low.
            On a motion signal-based action classification task, the performance using only 10% training data is very close to the optimal one.
            InFi can save 80% inference compuations while keeping 95% inference accuracy.
        </p>
        <figure>
            <img src="imgs/infi/har.PNG" width="100%"/> 
            <figcaption> InFi filters on Human Action Recognition (HAR) inference workloads. R denotes the ratio of training samples used. The "Randome" case labels each input randomly. </figcaption> 
        </figure>

        <b>Conclusion</b>
        <p>
            We study the input filtering problem and provide theoretical results on filterability.
            We propose the first end-to-end learnable framework that unifies both SKIP and REUSE mechanisms.
            Comprehensive evaluations confirm our proven results and show that InFi has wider applicability and outperforms strong baselines on inference accuracy and efficiency.
        </p>

        <b>Acknowledgments</b>
        <p>
            <i>I would like to thank Lan Zhang and Xiang-Yang Li who guided me through all aspects of this paper.
            And I would like to thank Fengxiang He for the help in the filterability analysis and Xueting Tong for testing InFi on Android devices.</i>
        </p>
      </section>
      <footer>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist" target="_blank" rel="noopener noreferrer">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
  </body>
</html>
